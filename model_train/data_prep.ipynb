{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREP STEP 0\n",
    "# to begin with, we have 10 total speakers, for each we have:\n",
    "# - 1 full clip of their real voice\n",
    "# - 19 sentence clips of their cloned voice reading the rainbow passage\n",
    "# - 12 sentence clips of their cloned voice reading two other passages\n",
    "#   TODO: what are we using for testing vs training? I think there is a \n",
    "#         misunderstanding with the data... I think intuitively, the\n",
    "#         \"novel\" data to use for testing is different voices. So, in this\n",
    "#         case, we either need more voices, or we can put aside 2 speakers\n",
    "#         for testing and 1 speaker for validating.\n",
    "\n",
    "# data directory structure\n",
    "# |-- raw_data\n",
    "# |   |-- Speaker 1 Real\n",
    "# |   |-- Speaker 1 Rainbow Passage\n",
    "# |   |-- Speaker 1 Test/Validation\n",
    "# |   |-- ...\n",
    "# |   |-- Speaker 10 Real\n",
    "# |   |-- Speaker 10 Rainbow Passage\n",
    "# |   |-- Speaker 10 Test/Validation\n",
    "\n",
    "\n",
    "\n",
    "# TODO: Zack's responsibility\n",
    "# PREP STEP 1\n",
    "# pick 2 speakers for test data\n",
    "# pick 1 speaker for validation data\n",
    "# take all data and use forced aligner charsiu to filter out all fricatives\n",
    "# store all clips of fricatives as wav files\n",
    "\n",
    "# data directory structure\n",
    "# |-- raw_data\n",
    "# |-- wav_data\n",
    "# |   |-- test\n",
    "# |   |   |-- clone\n",
    "# |   |   |   |-- s9_f1.wav \n",
    "# |   |   |   |-- s9_f2.wav \n",
    "# |   |   |   |-- ... \n",
    "# |   |   |   |-- s10_f1.wav \n",
    "# |   |   |   |-- s10_f2.wav \n",
    "# |   |   |   |-- ... \n",
    "# |   |   |-- natur\n",
    "# |   |   |   |-- s9_f1.wav \n",
    "# |   |   |   |-- s9_f2.wav \n",
    "# |   |   |   |-- ... \n",
    "# |   |   |   |-- s10_f1.wav \n",
    "# |   |   |   |-- s10_f2.wav \n",
    "# |   |   |   |-- ... \n",
    "# |   |-- train\n",
    "# |   |   |-- clone (clips for s1-s7)\n",
    "# |   |   |-- natur (clips for s1-s7)\n",
    "# |   |-- val\n",
    "# |   |   |-- clone (clips for s8)\n",
    "# |   |   |-- natur (clips for s8)\n",
    "\n",
    "\n",
    "\n",
    "# PREP STEP 2\n",
    "# take all wav files and convert to spectrograms, keep exact structure\n",
    "# use librosa to convert wav files to melspectrograms\n",
    "# manually normalize and convert to image, save\n",
    "\n",
    "# data directory structure\n",
    "# |-- raw_data\n",
    "# |-- wav_data\n",
    "# |-- spg_data\n",
    "# |   |-- test\n",
    "# |   |   |-- clone\n",
    "# |   |   |   |-- s9_f1.png\n",
    "# |   |   |   |-- s9_f2.png \n",
    "# |   |   |   |-- ... \n",
    "# |   |   |   |-- s10_f1.png \n",
    "# |   |   |   |-- s10_f2.png \n",
    "# |   |   |   |-- ... \n",
    "# |   |   |-- natur\n",
    "# |   |   |   |-- s9_f1.png\n",
    "# |   |   |   |-- s9_f2.png \n",
    "# |   |   |   |-- ... \n",
    "# |   |   |   |-- s10_f1.png \n",
    "# |   |   |   |-- s10_f2.wav \n",
    "# |   |   |   |-- ... \n",
    "# |   |-- train\n",
    "# |   |   |-- clone (clips for s1-s7)\n",
    "# |   |   |-- natur (clips for s1-s7)\n",
    "# |   |-- val\n",
    "# |   |   |-- clone (clips for s8)\n",
    "# |   |   |-- natur (clips for s8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import librosa\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organize directories ----------------------------------------------------\n",
    "cwd = os.getcwd()\n",
    "# stores test fricative waveform files for all sentences and speakers\n",
    "wav_test_dir = cwd + '/wav_data/test/'\n",
    "\n",
    "# stores train fricative waveform files for all sentences and speakers\n",
    "wav_train_dir = cwd + '/wav_data/train/'\n",
    "\n",
    "# stores val fricative waveform files for all sentences and speakers\n",
    "wav_val_dir = cwd + '/wav_data/val/'\n",
    "\n",
    "# stores test fricative spectrogram images for all sentences and speakers\n",
    "spg_test_dir = cwd + '/spg_data/test/'\n",
    "\n",
    "# stores train fricative spectrogram images for all sentences and speakers\n",
    "spg_train_dir = cwd + '/spg_data/train/'\n",
    "\n",
    "# stores val fricative spectrogram images for all sentences and speakers\n",
    "spg_val_dir = cwd + '/spg_data/val/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions --------------------------------------------------------\n",
    "def normalize_to_img(spect):\n",
    "  '''\n",
    "  this function normalizes all values in spectrogram to 0-255\n",
    "    INPUTS\n",
    "      spect   : spectrogram\n",
    "    OUTPUTS\n",
    "      normalized spect\n",
    "  '''\n",
    "  return (spect - spect.min()) / (spect.max() - spect.min()) *255\n",
    "\n",
    "\n",
    "# TODO: check if this spectrogram output is as expected\n",
    "# TODO: learn about how to choose spectrogram parameters\n",
    "# TODO: tangential to parameters, make sure that short waveforms will not\n",
    "#       affect spectrogram quality, or at least how do we choose params\n",
    "#       to properly scale the resolution of the spectrogram\n",
    "# TODO: check what librosa melspectrogram output is\n",
    "# TODO: look into if it is possible to save librosa.disaply output directly\n",
    "#       as image, in my inital search, this was not obvious\n",
    "# TODO: look into torchvision.transforms.spectrograms, might be better to\n",
    "#       just use this because I think it saves as tensor (is a tensor \n",
    "#       what we want essentially?)\n",
    "def get_spectrograms(wav_dir, spg_dir):\n",
    "  '''\n",
    "  this function creates mel spectrogram in db for all waveforms in a folder\n",
    "    INPUTS\n",
    "      wav_dir   : directory name of waveforms\n",
    "      spect_dir : directory name of where to store spectrograms\n",
    "    OUTPUTS\n",
    "      none\n",
    "  '''\n",
    "  for file in list(os.listdir(wav_dir)):\n",
    "    # load wav file\n",
    "    y, sr = librosa.load(wav_dir + file, sr=22050)\n",
    "\n",
    "    # convert to melspg in db\n",
    "    spect = librosa.feature.melspectrogram(y=y, sr=sr, hop_length=512)\n",
    "    spect = librosa.amplitude_to_db(spect, ref=np.max)\n",
    "\n",
    "    # remove single-dimension for clarity\n",
    "    # normalize to pixel val 0-255 and typecast to uint8\n",
    "    spect_img = normalize_to_img(spect).astype(np.uint8)\n",
    "\n",
    "    # flip to correct y-axis, frequencies from low -> high\n",
    "    spect_img = np.flip(spect_img)\n",
    "\n",
    "    # invert pixels s.t. more energy is represented by darker pixels\n",
    "    spect_img = 255-spect_img\n",
    "\n",
    "    # create image\n",
    "    im = Image.fromarray(spect_img)\n",
    "    # save under same filename\n",
    "    fname = file.split('.')\n",
    "    im.save(spg_dir + fname[0]+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get spg for all wav -----------------------------------------------------\n",
    "# for test data\n",
    "get_spectrograms(wav_test_dir+'clone/', spg_test_dir+'clone/')\n",
    "get_spectrograms(wav_test_dir+'natur/', spg_test_dir+'natur/')\n",
    "# for train data\n",
    "get_spectrograms(wav_train_dir+'clone/', spg_train_dir+'clone/')\n",
    "get_spectrograms(wav_train_dir+'natur/', spg_train_dir+'natur/')\n",
    "# for val data\n",
    "get_spectrograms(wav_val_dir+'clone/', spg_val_dir+'clone/')\n",
    "get_spectrograms(wav_val_dir+'natur/', spg_val_dir+'natur/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datasets and dataloaders -----------------------------------------\n",
    "train_ds = datasets.ImageFolder(root=spg_train_dir, \n",
    "                                transform=transforms.ToTensor())\n",
    "train_loader = DataLoader(train_ds, batch_size=64)\n",
    "\n",
    "val_ds = datasets.ImageFolder(root=spg_val_dir, \n",
    "                                transform=transforms.ToTensor())\n",
    "val_loader = DataLoader(val_ds, batch_size=64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
